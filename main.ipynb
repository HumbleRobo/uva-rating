{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17f631d7",
   "metadata": {},
   "source": [
    "# Use Uhunt API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf0c92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "class UhuntAPI:\n",
    "    def __init__(self):\n",
    "        self.base_url = 'https://uhunt.onlinejudge.org/api'\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update({'User-Agent': 'Mozilla/5.0'})\n",
    "    \n",
    "    def get_problems(self):\n",
    "        path = '/p'\n",
    "        url = f'{self.base_url}{path}'\n",
    "        response = self.session.get(url)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            columns = ['pid', 'num', 'title', 'dacu', 'mrun', 'mmem', 'nover', 'sube', 'noj', 'inq', 'ce', 'rf', 're', 'ole', 'tle', 'mle', 'wa', 'pe', 'ac', 'rtl', 'status', 'rej']\n",
    "            return pd.DataFrame(data, columns=columns)\n",
    "        else:\n",
    "            raise Exception(f\"Failed to fetch problems: {response.status_code}\")\n",
    "\n",
    "    def get_problem_submissions(self, pid, start_sbt=0, end_sbt=2147483647):\n",
    "        results = []\n",
    "        while True:\n",
    "            path = f'/p/subs/{pid}/{start_sbt}/{end_sbt}'\n",
    "            url = f'{self.base_url}{path}'\n",
    "            response = self.session.get(url)\n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                columns = ['sid', 'uid', 'pid', 'ver', 'lan', 'run', 'mem', 'rank', 'sbt', 'name', 'uname']\n",
    "                df = pd.DataFrame(data, columns=columns)\n",
    "                results.append(df)\n",
    "                end_sbt = df['sbt'].min()\n",
    "                if not data or end_sbt <= start_sbt:\n",
    "                    break\n",
    "            else:\n",
    "                raise Exception(f\"Failed to fetch submissions for problem {pid}: {response.status_code}\")\n",
    "        df = pd.concat(results, ignore_index=True)\n",
    "        df = df[df['sbt'] >= start_sbt]\n",
    "        df.sort_values(by='sbt', ascending=False, inplace=True)\n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "        return df\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a4fc9f",
   "metadata": {},
   "source": [
    "# Get Submission Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2f880740",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "try:\n",
    "    api = UhuntAPI()\n",
    "    problems_df = api.get_problems()\n",
    "    submissions_dir = 'submissions'\n",
    "    if not os.path.exists(submissions_dir):\n",
    "        os.makedirs(submissions_dir)\n",
    "    for index, row in problems_df.iterrows():\n",
    "        path = os.path.join(submissions_dir, f'{row[\"num\"]}.csv')\n",
    "        if os.path.exists(path):\n",
    "            prev_submissions_df = pd.read_csv(path)\n",
    "            last_submission_time = prev_submissions_df['sbt'].max()\n",
    "            submissions_df = api.get_problem_submissions(row['pid'], last_submission_time + 1)\n",
    "            submissions_df = pd.concat([prev_submissions_df, submissions_df], ignore_index=True)\n",
    "            submissions_df.drop_duplicates(subset=['sid'], keep='last', inplace=True)\n",
    "        else:\n",
    "            submissions_df = api.get_problem_submissions(row['pid'])\n",
    "        submissions_df.to_csv(path, index=False)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ff53b4",
   "metadata": {},
   "source": [
    "# Concatenate all submissions into a single CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b2c0d080",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "try:\n",
    "    submissions_dir = 'submissions'\n",
    "    all_submissions = []\n",
    "    for filename in os.listdir(submissions_dir):\n",
    "        if filename.endswith('.csv'):\n",
    "            path = os.path.join(submissions_dir, filename)\n",
    "            df = pd.read_csv(path)\n",
    "            all_submissions.append(df)\n",
    "    all_submissions_df = pd.concat(all_submissions, ignore_index=True)\n",
    "    all_submissions_df.drop_duplicates(subset=['sid'], keep='last', inplace=True)\n",
    "    all_submissions_df.sort_values(by=['sid'], ascending=True, inplace=True)\n",
    "    all_submissions_df.to_csv('all_submissions.csv', index=False)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3236cbb",
   "metadata": {},
   "source": [
    "# Get previous attempt count for each submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "43dc2e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "try:\n",
    "    # filename = 'test/all_submissions_1000.csv'\n",
    "    filename = 'all_submissions.csv'\n",
    "    df = pd.read_csv(filename)\n",
    "\n",
    "    first_ac_sid = df[df['ver'] == 90].groupby(['uid', 'pid'])['sid'].min().reset_index()\n",
    "    first_ac_sid.rename(columns={'sid': 'first_ac_sid'}, inplace=True)\n",
    "    df = df.merge(first_ac_sid, on=['uid', 'pid'], how='left')\n",
    "\n",
    "    attempt_df = df[\n",
    "        df['ver'].between(30, 90)\n",
    "        & ((df['first_ac_sid'].isna()) | (df['sid'] <= df['first_ac_sid']))\n",
    "    ]\n",
    "    prev_attempt_count = attempt_df.groupby(['uid', 'pid']).cumcount()\n",
    "    attempt_ac = attempt_df['ver'] == 90\n",
    "\n",
    "    attempt_df = attempt_df[['sid', 'uid', 'pid']]\n",
    "    attempt_df['prev_attempt_count'] = prev_attempt_count\n",
    "    attempt_df['ac'] = attempt_ac\n",
    "\n",
    "    final_attempt_idx = attempt_df.groupby(['uid', 'pid'])['prev_attempt_count'].idxmax()\n",
    "    attempt_df = attempt_df.loc[final_attempt_idx]\n",
    "    attempt_df.sort_values(by=['sid'], ascending=True, inplace=True)\n",
    "    attempt_df.to_csv('attempt_count.csv', index=False)\n",
    "\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833da691",
   "metadata": {},
   "source": [
    "# Simulate Elo rating calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832624e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import pow\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "\n",
    "class EloRating:\n",
    "    def __init__(self, initial_problem_rating=500, initial_user_rating=500, k=50, decay=0.9):\n",
    "        self.problem_ratings = defaultdict(lambda: initial_problem_rating)\n",
    "        self.user_ratings = defaultdict(lambda: initial_user_rating)\n",
    "        self.k = k\n",
    "        self.decay = decay\n",
    "    \n",
    "    def update_rating(self, problem_id, user_id, ac, prev_attempt_count):\n",
    "        problem_rating = self.problem_ratings[problem_id]\n",
    "        user_rating = self.user_ratings[user_id]\n",
    "        score = ac * pow(self.decay, prev_attempt_count)\n",
    "        rating_change_problem, rating_change_user = self.elo_rating_change(problem_rating, user_rating, 1-score, score, self.k)\n",
    "\n",
    "        self.problem_ratings[problem_id] += rating_change_problem\n",
    "        self.user_ratings[user_id] += rating_change_user\n",
    "\n",
    "    def elo_rating_change(self, rating_a, rating_b, score_a, score_b, k):\n",
    "        expected_score_a = 1 / (1 + 10 ** ((rating_b - rating_a) / 400))\n",
    "        expected_score_b = 1 / (1 + 10 ** ((rating_a - rating_b) / 400))\n",
    "        rating_change_a = k * (score_a - expected_score_a)\n",
    "        rating_change_b = k * (score_b - expected_score_b)\n",
    "        return rating_change_a, rating_change_b\n",
    "\n",
    "    def save_problem_ratings(self, filename):\n",
    "        api = UhuntAPI()\n",
    "        problems_df = api.get_problems()\n",
    "        problems_df['rating'] = problems_df['pid'].map(self.problem_ratings)\n",
    "        problems_df = problems_df[['pid', 'num', 'title', 'rating']]\n",
    "        problems_df.to_csv(filename, index=False)\n",
    "    \n",
    "    def save_user_ratings(self, filename):\n",
    "        users_df = pd.DataFrame(self.user_ratings.items(), columns=['uid', 'rating'])\n",
    "        users_df.to_csv(filename, index=False)\n",
    "\n",
    "try:\n",
    "    filename = 'attempt_count.csv'\n",
    "    df = pd.read_csv(filename)\n",
    "\n",
    "    # df = df[df['pid'] >= 941]\n",
    "    threshold_count = 5\n",
    "\n",
    "    df_count = df.groupby(['uid']).size().reset_index(name='count')\n",
    "    df = df.merge(df_count, on=['uid'], how='left')\n",
    "    df = df[df['count'] >= threshold_count]\n",
    "\n",
    "    elo_system = EloRating()\n",
    "    for index, row in df.iterrows():\n",
    "        elo_system.update_rating(row['pid'], row['uid'], row['ac'], row['prev_attempt_count'])\n",
    "    elo_system.save_problem_ratings('problem_ratings.csv')\n",
    "    elo_system.save_user_ratings('user_ratings.csv')\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
